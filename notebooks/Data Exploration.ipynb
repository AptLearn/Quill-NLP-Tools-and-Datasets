{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EATINGMEAT_BECAUSE_TRAIN = \"../data/interim/eatingmeat_because_large_train_withprompt.ndjson\"\n",
    "EATINGMEAT_BECAUSE_TEST = \"../data/interim/eatingmeat_because_large_test_withprompt.ndjson\"\n",
    "\n",
    "EATINGMEAT_BUT_TRAIN = \"../data/interim/eatingmeat_but_large_train_withprompt.ndjson\"\n",
    "EATINGMEAT_BUT_TEST = \"../data/interim/eatingmeat_but_large_test_withprompt.ndjson\"\n",
    "\n",
    "JUNKFOOD_BECAUSE_TRAIN = \"../data/interim/junkfood_because_train_withprompt.ndjson\"\n",
    "JUNKFOOD_BUT_TRAIN = \"../data/interim/junkfood_but_train_withprompt.ndjson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_FILE_BECAUSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a1fb19352d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtokens_because_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_FILE_BECAUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtokens_because_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_FILE_BECAUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAIN_FILE_BECAUSE' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "import ndjson\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def count_tokens(f):\n",
    "    with open(f) as i:\n",
    "        data = ndjson.load(i)\n",
    "        \n",
    "    tokens = Counter()\n",
    "    for item in data:\n",
    "        tokens.update([t.orth_ for t in nlp(item[\"text\"])])\n",
    "        \n",
    "    return tokens\n",
    "        \n",
    "\n",
    "tokens_because_train = count_tokens(TRAIN_FILE_BECAUSE)\n",
    "tokens_because_test = count_tokens(TEST_FILE_BECAUSE)\n",
    "\n",
    "tokens_but_train = count_tokens(TRAIN_FILE_BUT)\n",
    "tokens_but_test = count_tokens(TEST_FILE_BUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BECAUSE\n",
      "Number of tokens in train: 569\n",
      "Number of tokens in test: 294\n",
      "Unseen tokens in test: 63\n",
      "% of unseen tokens in test: 0.21428571428571427\n",
      "\n",
      "BUT\n",
      "Number of tokens in train: 936\n",
      "Number of tokens in test: 452\n",
      "Unseen tokens in test: 119\n",
      "% of unseen tokens in test: 0.26327433628318586\n"
     ]
    }
   ],
   "source": [
    "def compute_overlap(train, test):\n",
    "\n",
    "    print(\"Number of tokens in train:\", len(train))\n",
    "    print(\"Number of tokens in test:\", len(test))\n",
    "    \n",
    "    unseen = [t for t in test if not t in train]\n",
    "    print(\"Unseen tokens in test:\", len(unseen))\n",
    "    print(\"% of unseen tokens in test:\", len(unseen)/len(test))\n",
    "\n",
    "    \n",
    "print(\"BECAUSE\")\n",
    "compute_overlap(tokens_because_train, tokens_because_test)\n",
    "print(\"\")\n",
    "print(\"BUT\")\n",
    "compute_overlap(tokens_but_train, tokens_but_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating meat - but: 0.2536198693730758\n",
      "Eating meat - because: 0.2974968609449983\n",
      "Junk food - but: 0.3291589014804912\n",
      "Junk food - because: 0.4016385873271023\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def compute_label_overlap(data): \n",
    "    \n",
    "    tokens_by_label = defaultdict(lambda: set())\n",
    "    for item in data:\n",
    "        tokens = set([t.orth_ for t in nlp(item[\"text\"])])\n",
    "        tokens_by_label[item[\"label\"]].update(tokens)\n",
    "        \n",
    "    overlaps = []\n",
    "    for label1 in tokens_by_label:\n",
    "        for label2 in tokens_by_label:\n",
    "            tokenset1 = tokens_by_label[label1]\n",
    "            tokenset2 = tokens_by_label[label2]\n",
    "            overlap = len(tokenset1 & tokenset2)/len(tokenset1 | tokenset2)\n",
    "            overlaps.append(overlap)\n",
    "    \n",
    "    return sum(overlaps) / len(overlaps)\n",
    "\n",
    "def compute_label_overlap_from_file(f):\n",
    "    with open(f) as i:\n",
    "        data = ndjson.load(i)\n",
    "    overlap = compute_label_overlap(data)\n",
    "    return overlap\n",
    "\n",
    "print(\"Eating meat - but:\", compute_label_overlap_from_file(EATINGMEAT_BUT_TRAIN))\n",
    "print(\"Eating meat - because:\", compute_label_overlap_from_file(EATINGMEAT_BECAUSE_TRAIN))\n",
    "print(\"Junk food - but:\", compute_label_overlap_from_file(JUNKFOOD_BUT_TRAIN))\n",
    "print(\"Junk food - because:\", compute_label_overlap_from_file(JUNKFOOD_BECAUSE_TRAIN))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "count  49.000000\n",
      "mean    0.576889\n",
      "std     0.338906\n",
      "min     0.000000\n",
      "25%     0.176325\n",
      "50%     0.702399\n",
      "75%     0.810982\n",
      "max     0.945170\n",
      "Eating Meat - because 0.5768892926126091\n",
      "                0\n",
      "count  121.000000\n",
      "mean     0.622426\n",
      "std      0.230877\n",
      "min      0.000000\n",
      "25%      0.596339\n",
      "50%      0.669411\n",
      "75%      0.749399\n",
      "max      0.886755\n",
      "Eating Meat - but 0.622426429537375\n",
      "               0\n",
      "count  16.000000\n",
      "mean    0.143293\n",
      "std     0.097128\n",
      "min     0.000000\n",
      "25%     0.086045\n",
      "50%     0.162129\n",
      "75%     0.209390\n",
      "max     0.263503\n",
      "Junk food - because 0.14329315186519204\n",
      "               0\n",
      "count  49.000000\n",
      "mean    0.129789\n",
      "std     0.076973\n",
      "min     0.000000\n",
      "25%     0.079385\n",
      "50%     0.125557\n",
      "75%     0.161029\n",
      "max     0.278618\n",
      "Junk food - but 0.12978943185195838\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def compute_label_similarity(data):\n",
    "    texts_by_label = defaultdict(list)\n",
    "    for item in data:\n",
    "        text = item[\"text\"]\n",
    "        text = re.split(\", (but|because) \", text)[-1]\n",
    "        texts_by_label[item[\"label\"]].append(text)\n",
    "        \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    labels = list(texts_by_label.keys())\n",
    "    label_matrix = vectorizer.fit_transform([\" \".join(texts_by_label[label]) for label in labels])\n",
    "    \n",
    "    cosines = []\n",
    "    for i, label1 in enumerate(labels):\n",
    "        for j, label2 in enumerate(labels):\n",
    "            cosine = spatial.distance.cosine(label_matrix[i,:].todense(), label_matrix[j,:].todense())\n",
    "            cosines.append(cosine)\n",
    "\n",
    "    df = pd.DataFrame(cosines)\n",
    "    print(df.describe())\n",
    "            \n",
    "    return sum(cosines)/len(cosines)\n",
    "    \n",
    "def compute_label_similarity_from_file(f):\n",
    "    with open(f) as i:\n",
    "        data = ndjson.load(i)\n",
    "    overlap = compute_label_similarity(data)\n",
    "    return overlap\n",
    "\n",
    "\n",
    "print(\"Eating Meat - because\", compute_label_similarity_from_file(EATINGMEAT_BECAUSE_TRAIN))\n",
    "print(\"Eating Meat - but\", compute_label_similarity_from_file(EATINGMEAT_BUT_TRAIN))\n",
    "print(\"Junk food - because\", compute_label_similarity_from_file(JUNKFOOD_BECAUSE_TRAIN))\n",
    "print(\"Junk food - but\", compute_label_similarity_from_file(JUNKFOOD_BUT_TRAIN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
