{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLearn Subject Verb Agreement Error Detection 2\n",
    "\n",
    "This notebook is based off the original fragment detection notebook, but specific to detection of participle phrase fragments.\n",
    "\n",
    "As our training data we will use 799,675 correct sentences and, of a total 12,743,496 sentences with subject verb agreement errors, we will use a randomly chosen 799,675.\n",
    "\n",
    "The labels will be either a 1 or 0, where 1 indicates a sentence with a subject verb agreement error and 0 indicates there is no subject verb agreement errors.\n",
    "\n",
    "Because some libraries used require python 2.7, this jupyter notebook may not be able to run but it is used to document process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical\n",
    "import spacy\n",
    "import re\n",
    "from textstat.textstat import textstat\n",
    "from pattern.en import lexeme, tenses\n",
    "from pattern.en import pluralize, singularize\n",
    "import sqlite3\n",
    "import hashlib\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "conn = sqlite3.connect('db/mangled_agreement.db')\n",
    "cursor = conn.cursor()\n",
    "#from nltk.util import ngrams, trigrams\n",
    "#import csv\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This is kind of memory intensive don'tcha think?\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# add 0 label to correct sentences\n",
    "for row in cursor.execute(\"SELECT sentence FROM orignal_sentences\"):\n",
    "    texts.append(row[0].strip())\n",
    "    labels.append(0)\n",
    "\n",
    "# add 1 label to sentences with a subject verb agreement error, limit should match the number of original sentences\n",
    "for row in cursor.execute(\"SELECT sentence FROM mangled_sentences ORDER BY RANDOM() LIMIT 799675\"):\n",
    "    texts.append(row[0].strip())\n",
    "    labels.append(1)\n",
    "        \n",
    "print(texts[-10:])\n",
    "conn.close() # done with sqlite connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "combined = list(zip(texts,labels))\n",
    "random.shuffle(combined)\n",
    "\n",
    "texts[:], labels[:] = zip(*combined)\n",
    "print(texts[-10:])\n",
    "print(labels[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get verb phrase keys for sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb_phrases(sentence_doc):\n",
    "    \"\"\"\n",
    "    Returns an object like,\n",
    "    \n",
    "        [(1), (5,6,7)]\n",
    "        \n",
    "    where this means 2 verb phrases. a single verb at index 1, another verb phrase 5,6,7.  \n",
    "    \n",
    "     - Adverbs are not included.\n",
    "     - Infinitive phrases (and verb phrases that are subsets of infinitive phrases) are not included\n",
    "     \n",
    "    \"\"\" \n",
    "    pattern =  r'<VERB>*<ADV>*<VERB>+' #  r'<VERB>?<ADV>*<VERB>+' is suggested by textacy site\n",
    "    verb_phrases = textacy.extract.pos_regex_matches(sentence_doc, pattern)\n",
    "    sentence_str = sentence_doc.text\n",
    "    \n",
    "    index_2_word_no = {} # the starting position for each word to its number{0:0, 3:1, 7:2, 12:3}\n",
    "    for word in sentence_doc:\n",
    "        \n",
    "    result = [] # [(1), (5,6,7)] => 2 verb phrases. a single verb at index 1, another verb phrase 5,6,7\n",
    "    for vp in verb_phrases:\n",
    "        word_numbers = []\n",
    "        # return the index of 'could have been happily eating' from 'She could have been happily eating chowder'\n",
    "        str_idx = sentence_str.index(vp.text)\n",
    "        first_word = index_2_word_no[str_idx] # word number for first word of verb phrase\n",
    "        \n",
    "        x = first_word\n",
    "        if len(vp) > 1:\n",
    "            for verb_or_adverb in vp:\n",
    "                # filter out adverbs\n",
    "                if not verb_or_adverb.pos_ == 'ADV':\n",
    "                    word_numbers.append(x)\n",
    "                x += 1\n",
    "        else:\n",
    "            word_numbers.append(first_word)\n",
    "        \n",
    "        # filter out infinitive phrases\n",
    "        if ( (word_numbers[0] - 1) < 0) or (doc[word_numbers[0] - 1].text.lower() != 'to'):\n",
    "            result.append(word_numbers)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def singular_or_plural(word_string):\n",
    "    if word_string == singularize(word_string):\n",
    "        return 'SG'\n",
    "    else:\n",
    "        return 'PL'\n",
    "\n",
    "def sentence_to_keys(sentence):\n",
    "    doc = textacy.Doc(sentence, lang='en_core_web_lg')\n",
    "    \n",
    "    # [(1), (5,6,7)] => 2 verb phrases. a single verb at index 1, another verb phrase 5,6,7\n",
    "    verb_phrases = get_verb_phrases(doc)\n",
    "    \n",
    "    # doc = this could be my sentence\n",
    "    # doc_list = [this, -595002753822348241, 15488046584>THIS, my sentence]\n",
    "    # final_keys = [-595002753822348241:15488046584>THIS]\n",
    "    #\n",
    "    # doc = Jane is only here for tonight\n",
    "    # doc_list = [Jane, 13440080745121162>SG, only, here, for, tonight ]\n",
    "    # final_keys = [13440080745121162>SG]\n",
    "    doc_list = []\n",
    "    for word in doc:\n",
    "            \n",
    "        if word.pos_ == 'VERB':\n",
    "            tense_hash = hashlib.sha256((str(tenses(word.text)))).hexdigest()\n",
    "            verb_number_or_pronoun = ''\n",
    "            for child in word.children:\n",
    "                if child.dep_ == 'nsubj':\n",
    "                    if child.pos == 'PRON':\n",
    "                        verb_number_or_pronoun = child.text.upper()\n",
    "                    else:\n",
    "                        verb_number_or_pronoun = singular_or_plural(child.text)\n",
    "                    break\n",
    "        \n",
    "            doc_list.append(tense_hash + '>' + verb_number_or_pronoun)\n",
    "        else:\n",
    "            doc_list.append(word.text)\n",
    "    \n",
    "    # Get final keys\n",
    "    final_keys = []\n",
    "    for vp in verb_phrases:\n",
    "        vp_key_list = []\n",
    "        for word_no in vp:\n",
    "            vp_key_list.append(doc_list[word_no])\n",
    "        vp_key = ':'.join(vp_key_list)\n",
    "        final_keys.append(vp_key)\n",
    "    \n",
    "    return final_keys\n",
    "    \n",
    "    \n",
    "\n",
    "sentence_to_keys(texts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "\n",
    "for textString in texts:\n",
    "    c.update(sentence_to_keys(textString)))\n",
    "\n",
    "total_counts = c\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab[-1], ': ', total_counts[vocab[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the trigrams and index them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {n: i for i, n in enumerate(vocab)}## create the word-to-index dictionary here\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    wordVector = np.zeros(len(vocab))\n",
    "    for word in sentence_to_keys(text):\n",
    "        index = word2idx.get(word, None)\n",
    "        if index != None:\n",
    "            wordVector[index] += 1\n",
    "    return wordVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_vector('Donald, standing on the precipice, began to dance.')[:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(texts), len(vocab)), dtype=np.int_)\n",
    "for ii, text in enumerate(texts):\n",
    "    word_vectors[ii] = text_to_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the first 5 word vectors\n",
    "word_vectors[:5, :23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the data for TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = len(labels)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = int(records*test_fraction), int(records*(1-test_fraction))\n",
    "print(train_split, test_split)\n",
    "trainX, trainY = word_vectors[:train_split], to_categorical(labels[:train_split], 2)\n",
    "testX, testY = word_vectors[test_split:], to_categorical(labels[test_split:], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX[-1], trainY[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainY), len(testY), len(trainY) + len(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network building\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #### Your code ####\n",
    "    net = tflearn.input_data([None, len(vocab)])                          # Input\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU')      # Hidden\n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU')      # Hidden\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "    model = tflearn.DNN(net)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = csv.writer(open(\"../models/subjectverbagreementindex.csv\", \"w\"))\n",
    "for key, val in word2idx.items():\n",
    "    w.writerow([key, val])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/subject_verb_agreement_model.tfl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(sentence):\n",
    "    positive_prob = model.predict([text_to_vector(sentence)])[0][1]\n",
    "    print('Is this a participle phrase fragment?\\n {}'.format(sentence))\n",
    "    print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "          'Yes' if positive_prob > 0.5 else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Neglecting to recognize the horrors those people endure allow people to go to war more easily.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Katherine, gesticulating wildly and dripping in sweat, kissed him on the cheek.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Working far into the night in an effort to salvage her little boat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Working far into the night in an effort to salvage her little boat, she slowly grew tired.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Rushing to the rescue with his party.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Isobel was about thirteen now, and as pretty a girl, according to Buzzby, as you could meet with in any part of Britain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Being of a modest and retiring disposition, Mr. Hawthorne avoided publicity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Clambering to the top of a bridge, he observed a great rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Clambering to the top of a bridge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"He observed a great rainbow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Sitting on the iron throne, Joffry looked rather fat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Worrying that a meteor or chunk of space debris will conk her on the head.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Aunt Olivia always wears a motorcycle helmet, worrying that a meteor or chunk of space debris will conk her on the head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Affecting the lives of many students in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Quill was a miracle, affecting the lives of many students in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Standing on the edge of the cliff looking down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Emilia, standing on the edge of the cliff and looking down, began to weep.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Standing on the edge of the cliff and looking down, Emilia began to weep.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"Tired and needing sleep.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
